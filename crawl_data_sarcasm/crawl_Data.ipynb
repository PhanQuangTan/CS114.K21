{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawl_Data.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "154TnIbQHRYFQIEb2EKKfP-yn8hRcsn26",
      "authorship_tag": "ABX9TyM4RGMbs1iFSz+Kje6iFLii",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhanQuangTan/CS114.K21/blob/master/crawl_data_sarcasm/crawl_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gUL_m_6O5fl",
        "colab_type": "text"
      },
      "source": [
        "# **Mô tả**: \n",
        "Hiện nay, số lượng thông tin tăng theo thời gian. Kéo theo vấn nạn tin giả, tin châm biến, mỉa mai,... tràn lan internet. Và việc phân biệt và thu thập dữ liệu châm biến và không châm biếm từ các mạng xã hội khiến bộ dữ liệu trở nên không tốt trong nhiều ngữ cảnh.Từ đó, để khắc phục bộ dữ liệu thì dữ liệu sẽ được thu từ các tiêu đề của hai website.**Theonion** là trang website chuyên viết các bài viết châm biếm không chính thống cho các sự kiện hiện tại và các tiêu đề được thu thập từ hai mục: **News in Brief** và **News in Photos**. Và thu thập các tiêu đề bài viêt không châm biếm từ các tin tức thực sự của muc **News** của website **Huffpost UK**.\n",
        "Nội dung bộ dữ liệu: Mỗi dòng dữ liệu có 3 thuộc tính:\n",
        "\n",
        "1. **is_sarcastic**: Nếu tiêu đề được gán bằng **1** thì là tiêu đề thuộc dữ liệu châm biếm. Ngược lại, bằng **0** thì tiêu đề thuộc dữ liệu không châm biến.\n",
        "2. **headline**: Tiêu đề của bài viết.\n",
        "3. **article_link**: Đường dẫn của bài viết được thu thập tiêu đề.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIGjeHGmEVWU",
        "colab_type": "text"
      },
      "source": [
        "# **Ý tưởng:**\n",
        "Đầu tiên, chúng ta truy cập các mục **Newsb in brief** và **News in Photos** đối với **Theonion** và **News**  đối với HuffPost UK. Ở đây, đã có đầy đủ bài viết mới từ hai website.\n",
        "Để thực hiện được việc lấy tiêu đề và đường dẫn của từng bài viết ta sử dụng hai **thư viện Request và BeautifulSoup trong python** để cral data. \n",
        "1. Thư viện **Request**: dùng đề gửi yêu cầu đến url.\n",
        "2. Thư viện **BeautifulSoup**: dùng để lấy dữ liệu từ html(cụ thể ở đây là **headline** và **article_link** của từng bài viết).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IxkBgyTezi-",
        "colab_type": "text"
      },
      "source": [
        "# **Thu thập dữ liệu từ Theonion:**\n",
        "- Ý tưởng: Thu thập từ dữ liệu từ hai mục **Newsb in brief** và **News in Photos**.\n",
        "- Cách thu thập:\n",
        " + Từ trình duyệt, truy cập vào trang **Theonion** sau đó vào hai mục **News in brief** và **News in Photos**.\n",
        "+ Lấy đường các mục.\n",
        "+ Tạo vòng lặp for để kiểm soát lượng tiêu đề thu thập được, tránh quá ít hoặc quá nhiều.\n",
        "+ Kết nối đến các đường dẫn của các mục bằng thu viện request.\n",
        "+ Sử dụng BeautifulSoup để lấy source của website từ đường dẫn vừa kết nối và tìm đến đoạn code chứa: **Nancy Pelosi Calls Jamaal Bowman To Scold Him For Winning Primary**      \n",
        "```<a class=\"sc-1out364-0 hMndXN js_link\" data-ga=\"[[&quot;Story Type page click&quot;,&quot;stream post click - 0&quot;,&quot;https://politics.theonion.com/nancy-pelosi-calls-jamaal-bowman-to-scold-him-for-winni-1844151565&quot;,{&quot;metric19&quot;:1}]]\" href=\"https://politics.theonion.com/nancy-pelosi-calls-jamaal-bowman-to-scold-him-for-winni-1844151565?_ga=2.37173152.1892499765.1593073343-1357903050.1593073343\"><h2 class=\"sc-759qgu-0 cYlVdn cw4lnv-6 eXwNRE\">Nancy Pelosi Calls Jamaal Bowman To Scold Him For Winning Primary</h2></a>```\n",
        "- Sau khi lấy được soucre của web, ta thấy được dường dẫn bài viết nằm trong thẻ a với class=\"sc-1out364-0 hMndXN js_link\" và headline bài viết nằm trong thẻ h2 và class=\"sc-759qgu-0 cYlVdn cw4lnv-6 eXwNRE\"\n",
        "+ Tiếp theo ta dùng hàm fin_all trong thư viện BeautifulSoup để lấy các thẻ này ra.\n",
        "+ Sau đó, ta lấy đường dẫn và tên bài viết"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1kndp-d6aVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests \n",
        "from bs4 import BeautifulSoup\n",
        "# Đường link đến hai mục của trang Theonion\n",
        "url = ['https://www.theonion.com/c/news-in-brief', 'https://www.theonion.com/c/news-in-photos']\n",
        "# Mở file để ghi thêm vào cuối file, nếu không tìm thấy file sẽ tạo mới một file để ghi mới trong drive.\n",
        "with open('/content/data_sarcasm.json', 'a') as file:\n",
        "    # Vòng for để load hai đường link trong list url\n",
        "    for i in range(2):\n",
        "        url_1 = url[i]\n",
        "        url_2 = url_1\n",
        "        # Vòng for dùng để chuyển tiếp các trang web để lấy tiêu đề\n",
        "        for j in range(25):\n",
        "            # Gửi yêu cầu HTTP bằng request đến url_2.\n",
        "            r1 = requests.get(url_2)\n",
        "            # Truy cập r1 dưới dạng nhị phân.\n",
        "            coverpage = r1.content\n",
        "            # Thêm parser để phân tích HTML.\n",
        "            soup2 = BeautifulSoup(coverpage, 'html5lib')\n",
        "            # Phân tich HTML và tìm tất cả thẻ a, class_='sc-1out364-0 hMndXN js_link', đồng thời truy xuất đến text và href bên trong thẻ.\n",
        "            coverpage_news_1 = soup2.find_all('a', class_='sc-1out364-0 hMndXN js_link')\n",
        "  \n",
        "            t = 12 \n",
        "            # Vòng while để lấy tất cả tiêu đề và đường dẫn trong một trang web.\n",
        "            while(t <=50): \n",
        "                # Truy suất đường dẫn bài viết.\n",
        "                a = coverpage_news_1[t]['href']\n",
        "                # Truy suất tiêu đề bài viết\n",
        "                b = coverpage_news_1[t].get_text() \n",
        "   \n",
        "                # Ghi vào file.\n",
        "                file.write('{\"is_sarcastic\": 1, \"headline\": \"') \n",
        "                file.write(b)\n",
        "                file.write('\", \"article_link\": \"')\n",
        "                file.write(a)\n",
        "                file.write('\"}\\n')\n",
        "\n",
        "                t += 2\n",
        "                # Cập nhật trang web tiếp theo từ <a class=\"sc-1out364-0 hMndXN js_link\" href=\"?startTime=1592485200501\"> để tiếp tục lấy tiêu đề.\n",
        "                url_2 = url_1 + coverpage_news_1[51]['href'] \n",
        "    file.close()\n",
        "  "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxmUZR8WuWtz",
        "colab_type": "text"
      },
      "source": [
        "# **Thu thập dữ liệu từ HuffPost UK:**\n",
        "- Ý tưởng: Thu thập từ dữ liệu từ hai mục **News**.\n",
        "- Cách thu thập:\n",
        " + Từ trình duyệt, truy cập vào trang **HuffPost UK** sau đó vào hai mục **News**\n",
        "+ Lấy đường các mục.\n",
        "+ Tạo vòng lặp for để kiểm soát lượng tiêu đề thu thập được, tránh quá ít hoặc quá nhiều.\n",
        "+ Kết nối đến các đường dẫn của các mục bằng thu viện request.\n",
        "+ Sử dụng BeautifulSoup để lấy source của website từ đường dẫn vừa kết nối và tìm đến đoạn code chứa: **Anti-Mask Floridians Hurl Conspiracy Theories At Officials In Wild Public Meeting**      \n",
        "```<a class=\"card__link yr-card-headline\" href=\"/entry/florida-masks-conspiracy-theories_uk_5ef45be9c5b643f5b22f769b?utm_hp_ref=uk-news\" target=\"_self\" data-cpid=\"aa883209-23cf-30f7-8621-8d933455f53a\" data-rapid-pos=\"2\" data-ylk=\"cpos:1;pos:2;elm:hdln;g:aa883209-23cf-30f7-8621-8d933455f53a\" data-rapid-sec=\"{&quot;col2&quot;:&quot;col2&quot;}\" data-rapid-elm=\"hdln\" data-rapid-g=\"aa883209-23cf-30f7-8621-8d933455f53a\" data-rapid_p=\"3\" data-v9y=\"1\">Anti-Mask Floridians Hurl Conspiracy Theories At Officials In Wild Public Meeting</a>```\n",
        "- Sau khi lấy được soucre của web, ta thấy được dường dẫn bài viết nằm trong thẻ a class=\"card__link yr-card-headline\"\n",
        "+ Tiếp theo ta dùng hàm fin_all trong thư viện BeautifulSoup để lấy các thẻ này ra.\n",
        "+ Sau đó, ta lấy đường dẫn và tên bài viết"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMrT2hWkO0ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import Request , urlopen \n",
        "from bs4 import BeautifulSoup\n",
        "Data = []\n",
        "with open('/content/data_sarcasm.json', 'a') as file:\n",
        "\n",
        "  # Biến count dùng để crawl số tiêu đề cần thiết.\n",
        "  count = 1\n",
        "  # Vòng for dùng để chuyển tiếp các trang web để lấy tiêu đề\n",
        "  for i in range(1,34):  \n",
        "\n",
        "    # Định dạng lại đường đẫn các trang chứa tiêu đề trong phần news tương ướng với i.\n",
        "    url1 = 'https://www.huffingtonpost.co.uk/news/'+ str(i) +'/?guccounter=2'\n",
        "    # gửi request HTTP 1.1 đến url với header tự chỉnh \n",
        "    req = Request(url1, headers={'User-Agent': 'XYZ/3.0'}) \n",
        "    # Mở đường dẫn\n",
        "    page = urlopen(req,timeout=20) \n",
        "    # Thêm parser để phân tích HTML\n",
        "    soup = BeautifulSoup(page,\"html.parser\")\n",
        "     # Phân tich HTML và tìm tất cả thẻ a, class_='sc-1out364-0 hMndXN js_link', đồng thời truy xuất đến text và href bên trong thẻ.\n",
        "    headlines = soup.find_all('a',attrs={'class':'card__link yr-card-headline'})\n",
        "\n",
        "    # Vòng lặp dùng để lấy tiêu đề và đường dẫn bài viết.\n",
        "    for i in range(len(headlines)): \n",
        "      # Nếu đủ 1000 tiêu đề thì dùng lại.\n",
        "      if (count > 1000):\n",
        "        break\n",
        "      # Lấy tiêu đề bài viết.\n",
        "      headline = headlines[i].text\n",
        "      # Kiểm tra nếu tiêu đề đã được tải thì không tải lại.\n",
        "      if headline not in Data:\n",
        "        Data.append(headline)\n",
        "        # Lấy đường dẫn bài viết.\n",
        "        link = 'https://www.huffingtonpost.co.uk' + headlines[i]['href']\n",
        "        # Ghi vào file.\n",
        "        file.write('{\"is_sarcastic\": 0, \"headline\": \"') \n",
        "        file.write(headline)\n",
        "        file.write('\", \"article_link\": \"')\n",
        "        file.write(link)\n",
        "        file.write('\"}\\n')\n",
        "        count += 1\n",
        "  file.close()"
      ],
      "execution_count": 24,
      "outputs": []
    }
  ]
}