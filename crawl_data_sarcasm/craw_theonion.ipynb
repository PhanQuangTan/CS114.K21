{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "craw_theonion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "154TnIbQHRYFQIEb2EKKfP-yn8hRcsn26",
      "authorship_tag": "ABX9TyPm2n4PapBGprI4sbAyOYZX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhanQuangTan/CS114.K21/blob/master/crawl_data_sarcasm/craw_theonion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gUL_m_6O5fl",
        "colab_type": "text"
      },
      "source": [
        "# **Mô tả**: \n",
        "Hiện nay, số lượng thông tin tăng theo thời gian. Kéo theo vấn nạn tin giả, tin châm biến, mỉa mai,... tràn lan internet. Và việc phân biệt và thu thập dữ liệu châm biến và không châm biếm từ các mạng xã hội khiến bộ dữ liệu trở nên không tốt trong nhiều ngữ cảnh.Từ đó, để khắc phục bộ dữ liệu thì dữ liệu sẽ được thu từ các tiêu đề của hai website.**Theonion** là trang website chuyên viết các bài viết châm biếm không chính thống cho các sự kiện hiện tại và các tiêu đề được thu thập từ hai mục: **News in Brief** và **News in Photos**. Và thu thập các tiêu đề bài viêt không châm biếm từ các tin tức thực sự của muc **News** của website **Huffpost UK**.\n",
        " - Nội dung bộ dữ liệu: Mỗi dòng dữ liệu có 3 thuộc tính:\n",
        "\n",
        "1. **is_sarcastic**: Nếu tiêu đề được gán bằng **1** thì là tiêu đề thuộc dữ liệu châm biếm. Ngược lại, bằng **0** thì tiêu đề thuộc dữ liệu không châm biến.\n",
        "2. **headline**: Tiêu đề của bài viết.\n",
        "3. **article_link**: Đường dẫn của bài viết được thu thập tiêu đề.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIGjeHGmEVWU",
        "colab_type": "text"
      },
      "source": [
        "# **Ý tưởng:**\n",
        "Đầu tiên, chúng ta truy cập các mục **Newsb in brief** và **News in Photos** đối với **Theonion** và **News**  đối với HuffPost UK. Ở đây, đã có đầy đủ bài viết mới từ hai website.\n",
        "Để thực hiện được việc lấy tiêu đề và đường dẫn của từng bài viết ta sử dụng hai **thư viện Request và BeautifulSoup trong python** để cral data. \n",
        "1. Thư viện **Request**: dùng đề gửi yêu cầu đến url.\n",
        "2. Thư viện **BeautifulSoup**: dùng để lấy dữ liệu từ html(cụ thể ở đây là **headline** và **article_link** của từng bài viết).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP3S9gQ0uQTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests \n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35oZxNaU7Dqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "07027ef5-6d5c-4ed3-b1eb-8815fa14001b"
      },
      "source": [
        "%cd \"/content/drive/My Drive/\" "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IxkBgyTezi-",
        "colab_type": "text"
      },
      "source": [
        "# **Thu thập dữ liệu từ Theonion:**\n",
        "- Ý tưởng: Thu thập từ dữ liệu từ hai mục **Newsb in brief** và **News in Photos**.\n",
        "- Cách thu thập:\n",
        " + Từ trình duyệt, truy cập vào trang **Theonion** sau đó vào hai mục **News in brief** và **News in Photos**.\n",
        "+ Lấy đường các mục.\n",
        "+ Tạo vòng lặp for để kiểm soát lượng tiêu đề thu thập được, tránh quá ít hoặc quá nhiều.\n",
        "+ Kết nối đến các đường dẫn của các mục bằng thu viện request.\n",
        "+ Sử dụng BeautifulSoup để lấy source của website từ đường dẫn vừa kết nối và tìm đến đoạn code chứa: **Nancy Pelosi Calls Jamaal Bowman To Scold Him For Winning Primary**      \n",
        "```<a class=\"sc-1out364-0 hMndXN js_link\" data-ga=\"[[&quot;Story Type page click&quot;,&quot;stream post click - 0&quot;,&quot;https://politics.theonion.com/nancy-pelosi-calls-jamaal-bowman-to-scold-him-for-winni-1844151565&quot;,{&quot;metric19&quot;:1}]]\" href=\"https://politics.theonion.com/nancy-pelosi-calls-jamaal-bowman-to-scold-him-for-winni-1844151565?_ga=2.37173152.1892499765.1593073343-1357903050.1593073343\"><h2 class=\"sc-759qgu-0 cYlVdn cw4lnv-6 eXwNRE\">Nancy Pelosi Calls Jamaal Bowman To Scold Him For Winning Primary</h2></a>```\n",
        "- Sau khi lấy được soucre của web, ta thấy được dường dẫn bài viết nằm trong thẻ a với class=\"sc-1out364-0 hMndXN js_link\" và headline bài viết nằm trong thẻ h2 và class=\"sc-759qgu-0 cYlVdn cw4lnv-6 eXwNRE\"\n",
        "+ Tiếp theo ta dùng hàm fin_all trong thư viện BeautifulSoup để lấy các thẻ này ra.\n",
        "+ Sau đó, ta lấy đường dẫn và tên bài viết"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1kndp-d6aVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://www.theonion.com/c/news-in-brief\" # Gán đường dẫn cho biến url.\n",
        "url_1 = url\n",
        "with open('/content/drive/My Drive/data_sarcasm_1.json', 'a') as file: # Mở file để ghi thêm vào cuối file, nếu không tìm thấy file sẽ tạo mới một file để ghi mới trong drive.\n",
        "\n",
        "  for i in range(26): # Vòng lặp kiểm soát việc crawl bao nhiêu tiêu đề\n",
        "  \n",
        "    r1 = requests.get(url_1) # Gửi yêu cầu HTTP bằng request.\n",
        "    coverpage = r1.content # Truy cập r1 dưới dạng nhị phân.\n",
        "    #soup1 = BeautifulSoup(coverpage, 'lxml') # Thêm parser để phân tích HTML \n",
        "    soup2 = BeautifulSoup(coverpage, 'html5lib') # Thêm parser để phân tích HTML\n",
        "    #coverpage_news = soup1.find_all('h2', class_= \"sc-759qgu-0 cYlVdn cw4lnv-6 eXwNRE\") # Phân tích HTML và tìm đến thẻ h2, class_= \"sc-759qgu-0 cYlVdn cw4lnv-6 eXwNRE\"\n",
        "    coverpage_news_1 = soup2.find_all('a', class_='sc-1out364-0 hMndXN js_link') # Phân tich HTML và tìm tất cả thẻ a, class_='sc-1out364-0 hMndXN js_link'\n",
        "  \n",
        "    t = 12 \n",
        "    while(t <=50): # Vòng while đề lấy tất cả tiêu đề và đường dẫn trong một trang web.\n",
        "      a = coverpage_news_1[t]['href'] # Truy suất đường dẫn bài viết.\n",
        "      b = coverpage_news_1[t].get_text() # Truy suất tiêu đề bài viết\n",
        "   \n",
        "    \n",
        "      file.write('{\"is_sarcastic\": 1, \"headline\": \"') # Ghi vào file.\n",
        "      file.write(b)\n",
        "      file.write('\", \"article_link\": \"')\n",
        "      file.write(a)\n",
        "      file.write('\"}\\n')\n",
        "\n",
        "      t += 2\n",
        "\n",
        "    url_1 = url + coverpage_news_1[51]['href'] # Cập nhật trang web tiếp theo từ <a class=\"sc-1out364-0 hMndXN js_link\" href=\"?startTime=1592485200501\"> để tiếp tục lấy tiêu đề.\n",
        "  file.close()\n",
        "  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEPb9gEGFkYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://www.theonion.com/c/news-in-photos\"\n",
        "url_1 = url\n",
        "with open('/content/drive/My Drive/data_sarcasm_1.json', 'a') as file:\n",
        "\n",
        "  for i in range(26):\n",
        "  \n",
        "    r1 = requests.get(url_1)\n",
        "    coverpage = r1.content\n",
        "    #soup1 = BeautifulSoup(coverpage, 'lxml')\n",
        "    soup2 = BeautifulSoup(coverpage, 'html5lib')\n",
        "    #coverpage_news = soup1.find_all('h2', class_= \"sc-759qgu-0 cYlVdn cw4lnv-6 eXwNRE\")\n",
        "    coverpage_news_1 = soup2.find_all('a', class_='sc-1out364-0 hMndXN js_link')\n",
        "  \n",
        "    t = 12\n",
        "    while(t <=50):\n",
        "      a = coverpage_news_1[t]['href']\n",
        "      b = coverpage_news_1[t].get_text()\n",
        "   \n",
        "    \n",
        "      file.write('{\"is_sarcastic\": 1, \"headline\": \"')\n",
        "      file.write(b)\n",
        "      file.write('\", \"article_link\": \"')\n",
        "      file.write(a)\n",
        "      file.write('\"}\\n')\n",
        "\n",
        "      t += 2\n",
        "    url_1 = url + coverpage_news_1[51]['href']\n",
        "  file.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxmUZR8WuWtz",
        "colab_type": "text"
      },
      "source": [
        "# **Thu thập dữ liệu từ HuffPost UK:**\n",
        "- Ý tưởng: Thu thập từ dữ liệu từ hai mục **News**.\n",
        "- Cách thu thập:\n",
        " + Từ trình duyệt, truy cập vào trang **HuffPost UK** sau đó vào hai mục **News**\n",
        "+ Lấy đường các mục.\n",
        "+ Tạo vòng lặp for để kiểm soát lượng tiêu đề thu thập được, tránh quá ít hoặc quá nhiều.\n",
        "+ Kết nối đến các đường dẫn của các mục bằng thu viện request.\n",
        "+ Sử dụng BeautifulSoup để lấy source của website từ đường dẫn vừa kết nối và tìm đến đoạn code chứa: **Anti-Mask Floridians Hurl Conspiracy Theories At Officials In Wild Public Meeting**      \n",
        "```<a class=\"card__link yr-card-headline\" href=\"/entry/florida-masks-conspiracy-theories_uk_5ef45be9c5b643f5b22f769b?utm_hp_ref=uk-news\" target=\"_self\" data-cpid=\"aa883209-23cf-30f7-8621-8d933455f53a\" data-rapid-pos=\"2\" data-ylk=\"cpos:1;pos:2;elm:hdln;g:aa883209-23cf-30f7-8621-8d933455f53a\" data-rapid-sec=\"{&quot;col2&quot;:&quot;col2&quot;}\" data-rapid-elm=\"hdln\" data-rapid-g=\"aa883209-23cf-30f7-8621-8d933455f53a\" data-rapid_p=\"3\" data-v9y=\"1\">Anti-Mask Floridians Hurl Conspiracy Theories At Officials In Wild Public Meeting</a>```\n",
        "- Sau khi lấy được soucre của web, ta thấy được dường dẫn bài viết nằm trong thẻ a class=\"card__link yr-card-headline\"\n",
        "+ Tiếp theo ta dùng hàm fin_all trong thư viện BeautifulSoup để lấy các thẻ này ra.\n",
        "+ Sau đó, ta lấy đường dẫn và tên bài viết"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMrT2hWkO0ep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from urllib.request import Request , urlopen \n",
        "from bs4 import BeautifulSoup\n",
        "Data = []\n",
        "with open('/content/drive/My Drive/data_sarcasm_1.json', 'a') as file:\n",
        "  for i in range(1,35):  # Vòng lặp kiểm soát việc crawl bao nhiêu tiêu đề\n",
        "    url1 = 'https://www.huffingtonpost.co.uk/news/'+ str(i) +'/?guccounter=2' # Định dạng đường đẫn các trang trong phần news tương ướng với i.\n",
        "    req = Request(url1, headers={'User-Agent': 'XYZ/3.0'}) # gửi request HTTP 1.1 đến url với header tự chỉnh \n",
        "    page = urlopen(req,timeout=10) # Mở đường dẫn\n",
        "    soup = BeautifulSoup(page,\"html.parser\") # Thêm parser để phân tích HTML\n",
        "    headlines = soup.find_all('a',attrs={'class':'card__link yr-card-headline'}) # Phân tich HTML và tìm tất cả thẻ a, class_='sc-1out364-0 hMndXN js_link'\n",
        "\n",
        "    for i in range(len(headlines)): # Vòng lặp dùng để lấy tiêu đề và đường dẫn bài viết.\n",
        "      headline = headlines[i].text # Lấy tiêu đề bài viết.\n",
        "      #print(headline)\n",
        "      if headline not in Data: # Kiểm tra lặp.\n",
        "        Data.append(headline)\n",
        "        link = 'https://www.huffingtonpost.co.uk' + headlines[i]['href'] # Lấy đường dẫn bài viết.\n",
        "      #print(link)\n",
        "\n",
        "        file.write('{\"is_sarcastic\": 0, \"headline\": \"') # Ghi vào file.\n",
        "        file.write(headline)\n",
        "        file.write('\", \"article_link\": \"')\n",
        "        file.write(link)\n",
        "        file.write('\"}\\n')\n",
        "  file.close()"
      ],
      "execution_count": 5,
      "outputs": []
    }
  ]
}